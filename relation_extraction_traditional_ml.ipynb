{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run if not already installed\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install gensim\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Clint\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Clint\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Clint\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Clint\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained word2vec model\n",
    "# !!might take a while!!\n",
    "import gensim.downloader as api\n",
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Read and return the lines from a file.\n",
    "\n",
    "    Parameters:\n",
    "    filename: The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    A list of lines read from the file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r',  encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "        return lines\n",
    "    \n",
    "    \n",
    "def create_relation_dict(line):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of entity relations from a line.\n",
    "\n",
    "    Parameters:\n",
    "    line: A string containing triplets of (entity, related entity, relation) separated by '|'.\n",
    "    e.g. Chromecast ; Google ; manufacturer | Chromecast ; Google ; developer\n",
    "\n",
    "    Returns:\n",
    "    A dictionary where keys are tuples of the entities in the triplet and values are lists of relations between them.\n",
    "    e.g. (Chromecast, Google) : [manufacturer, developer]\n",
    "    \"\"\"\n",
    "    relation = {}\n",
    "    triplets = line.split('|') # split triplets\n",
    "    for triplet in triplets:\n",
    "        items = triplet.strip().split(';') # split entity relations\n",
    "        key = tuple([item.strip() for item in items[:2]]) # make a key out of a tuple of the entities\n",
    "        value = items[2].strip() # get relation\n",
    "        if key in relation:\n",
    "            relation[key].append(value)\n",
    "        else:\n",
    "            relation[key] = [value]\n",
    "    return relation\n",
    "\n",
    "def read_triplets(filename):\n",
    "    \"\"\"\n",
    "    Read triplets from a file and create relation dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    filename: The path to the file containing triplets.\n",
    "\n",
    "    Returns:\n",
    "    A list of relation dictionaries created from the triplets in the file.\n",
    "    \"\"\"\n",
    "    lines = read_file(filename)\n",
    "    return [create_relation_dict(line) for line in lines]\n",
    "\n",
    "def extract_X_Y(sentences, relation_dicts):\n",
    "    \"\"\"\n",
    "    Generate X and Y data from sentences and relation dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    sentences: A list of sentences.\n",
    "    relation_dicts: A list of relation dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing X data (input data) and Y data (output data) in the form of numpy arrays, \n",
    "    along with the MultiLabelBinarizer object for Y data.\n",
    "\n",
    "    X data: Each row represents a combination of a sentence and a key from the relation dictionary.\n",
    "    Y data: Each row represents the corresponding values from the relation dictionary for the respective sentence and key.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(sentences) != len(relation_dicts):\n",
    "        raise Exception(\"sentences and relation files do not match\")\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for key in relation_dicts[i].keys():\n",
    "            X.append([sentence]  + list(key)) # lines in X of the format [sentence, entity, related entity]\n",
    "            Y.append(relation_dicts[i][key])\n",
    "    # Convert list of target labels to binary matrix, where each column represents whether that label is applicable or not \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y = mlb.fit_transform(Y)\n",
    "    return np.array(X), np.array(Y), mlb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product/material produced',\n",
       " 'manufacturer',\n",
       " 'distributed by',\n",
       " 'industry',\n",
       " 'position held',\n",
       " 'original broadcaster',\n",
       " 'owned by',\n",
       " 'founded by',\n",
       " 'distribution format',\n",
       " 'headquarters location',\n",
       " 'stock exchange',\n",
       " 'currency',\n",
       " 'parent organization',\n",
       " 'chief executive officer',\n",
       " 'director/manager',\n",
       " 'owner of',\n",
       " 'operator',\n",
       " 'member of',\n",
       " 'employer',\n",
       " 'chairperson',\n",
       " 'platform',\n",
       " 'subsidiary',\n",
       " 'legal form',\n",
       " 'publisher',\n",
       " 'developer',\n",
       " 'brand',\n",
       " 'business division',\n",
       " 'location of formation',\n",
       " 'creator']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at possible labels\n",
    "labels = read_file(\"./datasets/relations.txt\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 29 sentences: 5700 relation_dicts: 5700\n",
      "X_train shape: (7070, 3) Example X_train data: ['NEW YORK (Reuters) - Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.'\n",
      " 'Apple Inc' 'Steve Jobs']\n",
      "Y_train shape: (7070, 29) Example Y_train data: [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Read Training data\n",
    "\n",
    "train_sentences = read_file(\"./datasets/train.sent\")\n",
    "train_relation_dicts = read_triplets(\"./datasets/train.tup\")\n",
    "\n",
    "# Extract X and Y from training data files\n",
    "X_train, Y_train, mlb = extract_X_Y(sentences=train_sentences, relation_dicts=train_relation_dicts)\n",
    "\n",
    "# Print some information about the data\n",
    "print(\"labels:\", len(labels), \"sentences:\", len(train_sentences), \"relation_dicts:\", len(train_relation_dicts))\n",
    "print(\"X_train shape:\", X_train.shape, \"Example X_train data:\", X_train[0])\n",
    "print(\"Y_train shape:\", Y_train.shape, \"Example Y_train data:\", Y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_between_entities(row):\n",
    "    \"\"\"\n",
    "    Calculate the tokens between two entities in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    row: A list containing the sentence and two entities.\n",
    "\n",
    "    Returns:\n",
    "    The tokens between the two entities.\n",
    "    \"\"\"\n",
    "    sentence_tokens = word_tokenize(row[0])\n",
    "    entity1_tokens = word_tokenize(row[1])\n",
    "    entity2_tokens = word_tokenize(row[2])\n",
    "    if sentence_tokens.index(entity1_tokens[-1]) < sentence_tokens.index(entity2_tokens[0]): #entity1 comes before entity2\n",
    "        from_index = sentence_tokens.index(entity1_tokens[-1])\n",
    "        to_index = sentence_tokens.index(entity2_tokens[0])\n",
    "    else: #entity2 comes before entity1\n",
    "        from_index = sentence_tokens.index(entity2_tokens[-1]) \n",
    "        to_index = sentence_tokens.index(entity1_tokens[0])\n",
    "    words_between = sentence_tokens[from_index + 1:to_index]\n",
    "    return ' '.join(words_between)\n",
    "\n",
    "def entity_neighbourhood(sentence, entity, window_size=3):\n",
    "    \"\"\"\n",
    "    Extract the tokens around a specific entity in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    sentence: The input sentence.\n",
    "    entity: The entity to extract tokens around.\n",
    "    window_size: The size of the window around the entity.\n",
    "\n",
    "    Returns:\n",
    "    The tokens around the specified entity (of a certain window size, inclusive of the entity)\n",
    "    \"\"\"\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    entity_tokens = word_tokenize(entity)\n",
    "    \n",
    "    entity_start_index = sentence_tokens.index(entity_tokens[0]) #locate where first token of entity lies\n",
    "    entity_end_index = sentence_tokens.index(entity_tokens[-1]) # locate where the last token lies\n",
    "    \n",
    "    start_index = max(0, entity_start_index - window_size)\n",
    "    end_index = min(len(sentence_tokens), entity_end_index + window_size + 1)\n",
    "    \n",
    "    tokens_around_entity = sentence_tokens[start_index:end_index]\n",
    "    \n",
    "    return ' '.join(tokens_around_entity)\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(X):\n",
    "    \"\"\"\n",
    "    Preprocess the input data by extracting features.\n",
    "\n",
    "    Parameters:\n",
    "    X: Input data containing sentences and entities.\n",
    "\n",
    "    Returns:\n",
    "    Processed data - extract entity neighbourhoods and tokens between entities\n",
    "    \"\"\"\n",
    "    \n",
    "    # get tokens between entities\n",
    "    tokens_between_entities= np.array([get_tokens_between_entities(row) for row in X])\n",
    "    # get entity neighbourhoods\n",
    "    entity1_neighbourhood = np.array([entity_neighbourhood(row[0], row[1]) for row in X])\n",
    "    entity2_neighbourhood = np.array([entity_neighbourhood(row[0], row[2]) for row in X])\n",
    "    \n",
    "    # add as new features to X\n",
    "    X = np.hstack((X, tokens_between_entities.reshape(-1, 1)))\n",
    "    X = np.hstack((X, entity1_neighbourhood.reshape(-1, 1)))\n",
    "    X = np.hstack((X, entity2_neighbourhood.reshape(-1, 1)))\n",
    "    X = pd.DataFrame(X)\n",
    "    X.columns = ['sentence', 'entity1', 'entity2', 'tokens_between_entities', 'entity1_neighbourhood', 'entity2_neighbourhood']\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>tokens_between_entities</th>\n",
       "      <th>entity1_neighbourhood</th>\n",
       "      <th>entity2_neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK (Reuters) - Apple Inc Chief Executive...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Steve Jobs</td>\n",
       "      <td>Chief Executive</td>\n",
       "      <td>Reuters ) - Apple Inc Chief Executive Steve</td>\n",
       "      <td>Inc Chief Executive Steve Jobs sought to soothe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last week, Citigroup Inc's ( C.N ) Chief Execu...</td>\n",
       "      <td>Vikram Pandit</td>\n",
       "      <td>Citigroup</td>\n",
       "      <td>Inc 's ( C.N ) Chief Executive</td>\n",
       "      <td>) Chief Executive Vikram Pandit said that he</td>\n",
       "      <td>Last week , Citigroup Inc 's (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lehman Brothers LEH.N shares fell sharply on M...</td>\n",
       "      <td>Lehman Brothers</td>\n",
       "      <td>investment bank</td>\n",
       "      <td>LEH.N shares fell sharply on Monday on specula...</td>\n",
       "      <td>Lehman Brothers LEH.N shares fell</td>\n",
       "      <td>speculation that the investment bank could be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lehman Brothers LEH.N shares fell sharply on M...</td>\n",
       "      <td>Lehman Brothers</td>\n",
       "      <td>investment</td>\n",
       "      <td>LEH.N shares fell sharply on Monday on specula...</td>\n",
       "      <td>Lehman Brothers LEH.N shares fell</td>\n",
       "      <td>speculation that the investment bank could be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Franz told Reuters that Fiat Chief Executive S...</td>\n",
       "      <td>Sergio Marchionne</td>\n",
       "      <td>Fiat</td>\n",
       "      <td>Chief Executive</td>\n",
       "      <td>Fiat Chief Executive Sergio Marchionne had sai...</td>\n",
       "      <td>told Reuters that Fiat Chief Executive Sergio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence            entity1  \\\n",
       "0  NEW YORK (Reuters) - Apple Inc Chief Executive...          Apple Inc   \n",
       "1  Last week, Citigroup Inc's ( C.N ) Chief Execu...      Vikram Pandit   \n",
       "2  Lehman Brothers LEH.N shares fell sharply on M...    Lehman Brothers   \n",
       "3  Lehman Brothers LEH.N shares fell sharply on M...    Lehman Brothers   \n",
       "4  Franz told Reuters that Fiat Chief Executive S...  Sergio Marchionne   \n",
       "\n",
       "           entity2                            tokens_between_entities  \\\n",
       "0       Steve Jobs                                    Chief Executive   \n",
       "1        Citigroup                     Inc 's ( C.N ) Chief Executive   \n",
       "2  investment bank  LEH.N shares fell sharply on Monday on specula...   \n",
       "3       investment  LEH.N shares fell sharply on Monday on specula...   \n",
       "4             Fiat                                    Chief Executive   \n",
       "\n",
       "                               entity1_neighbourhood  \\\n",
       "0        Reuters ) - Apple Inc Chief Executive Steve   \n",
       "1       ) Chief Executive Vikram Pandit said that he   \n",
       "2                  Lehman Brothers LEH.N shares fell   \n",
       "3                  Lehman Brothers LEH.N shares fell   \n",
       "4  Fiat Chief Executive Sergio Marchionne had sai...   \n",
       "\n",
       "                               entity2_neighbourhood  \n",
       "0    Inc Chief Executive Steve Jobs sought to soothe  \n",
       "1                     Last week , Citigroup Inc 's (  \n",
       "2  speculation that the investment bank could be ...  \n",
       "3      speculation that the investment bank could be  \n",
       "4      told Reuters that Fiat Chief Executive Sergio  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preprocessed = preprocessing(X_train)\n",
    "X_train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lexical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Entity distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDistance(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer class for calculating the distance between the given two entities in a sentence.\n",
    "    \n",
    "    Parameters:\n",
    "    absolute_values:  whether to return absolute distance values or not\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, absolute_values=False):\n",
    "        self.absolute_values = absolute_values\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        distance_matrix = [[self.entity_distance(row['sentence'], row['entity1'], row['entity2'])] for index, row in X.iterrows()]\n",
    "        distance_matrix_scaled = StandardScaler().fit_transform(distance_matrix)\n",
    "        return csr_matrix(distance_matrix_scaled) # convert to sparse coded matrix\n",
    "    \n",
    "    def entity_distance(self, sentence, entity1, entity2):\n",
    "        \"\"\"\n",
    "        Calculate the distance (in number of tokens) between two entities in a sentence.\n",
    "\n",
    "        Parameters:\n",
    "        sentence: The input sentence.\n",
    "        entity1: The first entity.\n",
    "        entity2: The second entity.\n",
    "\n",
    "        Returns:\n",
    "        The distance between the two entities.\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(sentence)\n",
    "        if self.absolute_values:\n",
    "            return abs(tokens.index(word_tokenize(entity2)[0]) - tokens.index(word_tokenize(entity1)[0]))\n",
    "        else:\n",
    "            return tokens.index(word_tokenize(entity2)[0]) - tokens.index(word_tokenize(entity1)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7070x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7070 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_distances = EntityDistance().fit_transform(X_train_preprocessed)\n",
    "entity_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Tf-idf Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag_map():\n",
    "    \"\"\"\n",
    "    Returns a mapping of POS tags to WordNet tags.\n",
    "    \n",
    "    Returns:\n",
    "    Mapping of POS tags to WordNet tags.\n",
    "    \"\"\"\n",
    "    pos_tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    pos_tag_map['J'] = wn.ADJ\n",
    "    pos_tag_map['V'] = wn.VERB\n",
    "    pos_tag_map['R'] = wn.ADV\n",
    "    return pos_tag_map\n",
    "\n",
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    custom tokenizer that \n",
    "    - tokenizes the input text\n",
    "    - converts to lowercase\n",
    "    - removes stopwords, \n",
    "    - LEMMATIZES based on pos-tags\n",
    "\n",
    "    Parameters:\n",
    "    text: Input text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    List of tokens after preprocessing\n",
    "    \"\"\"\n",
    "    text = text.lower() #lowercase\n",
    "    tokens = word_tokenize(text) #tokenize\n",
    "    pos_tags = pos_tag(tokens) # get pos_tags\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = []\n",
    "    pos_tag_map = get_pos_tag_map()\n",
    "    for token, tag in pos_tags:\n",
    "        if token.lower() not in stopwords.words('english'):\n",
    "            token = lemmatizer.lemmatize(token, pos=pos_tag_map[tag[0]])\n",
    "            # Remove punctuations\n",
    "            token = re.sub(r'[^\\w\\s]', '', token)\n",
    "            if token:  # if token is not empty\n",
    "                tokens.append(token)\n",
    "    return tokens\n",
    "  \n",
    "\n",
    "\n",
    "class TfidfFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer class for extracting TF-IDF vectorization of a sentence using TfidfVectorizer.\n",
    "    \n",
    "    Parameters:\n",
    "    max_features: Maximum number of features to consider. Takes top n most frequent words as features\n",
    "    ngram_range: Range for n-grams to consider.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_features=None, ngram_range=(1,3)):\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # we use a custom tokenzier because the default TfidfVectorizer does not lemmatize\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer, max_features=self.max_features, ngram_range=self.ngram_range)\n",
    "        sentences = X['sentence']\n",
    "        self.tfidf_vectorizer.fit(sentences)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sentences = X['sentence']\n",
    "        return self.tfidf_vectorizer.transform(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7070x228032 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 620774 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf = TfidfFeatures().fit_transform(X_train_preprocessed)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf vectorization with upto 3 n-grams leads to a dimension of 228,032. This is too much and very sparse. We will use the top 3000 most frequent terms for vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Syntactic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_pos_features(self, X):\n",
    "        pos_features_list = []\n",
    "        for sentence in X:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            pos_features = {}\n",
    "            for word, tag in pos_tags:\n",
    "                pos_features[tag] = pos_features.get(tag, 0) + 1\n",
    "            pos_features_list.append(pos_features)\n",
    "        return pos_features_list\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        pos_features = self.extract_pos_features(X['sentence'])\n",
    "        self.dict_vectorizer = DictVectorizer(sparse=True)\n",
    "        self.dict_vectorizer.fit(pos_features)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pos_features = self.extract_pos_features(X['sentence'])\n",
    "        pos_features_vectorized = self.dict_vectorizer.transform(pos_features)\n",
    "        return pos_features_vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7070x44 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 103586 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos_vectorized = POSFeatures().fit_transform(X_train_preprocessed)\n",
    "X_pos_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parser(sentence, entity):\n",
    "    entity = entity.strip().replace(\"-\",\" \").split(\" \")[0]\n",
    "    doc = nlp(sentence)\n",
    "    entity_token = None\n",
    "    for token in doc:\n",
    "        if token.text == entity:\n",
    "            entity_token = token\n",
    "    if entity_token is None:\n",
    "        raise Exception(\"Entity not present in the sentence\")\n",
    "    return entity_token\n",
    "    \n",
    "\n",
    "class ShortestPathFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_common_ancestor(self, entity1_ancestors, entity2_ancestors):\n",
    "        common_ancestor_index = 0\n",
    "        for i in range(1, min(len(entity1_ancestors), len(entity2_ancestors))):\n",
    "            if entity1_ancestors[i] == entity2_ancestors[i]:\n",
    "                common_ancestor_index = i\n",
    "            else:\n",
    "                break\n",
    "        return common_ancestor_index\n",
    "\n",
    "    def calculate_shortest_path(self, sentence, entity1, entity2):\n",
    "        try:\n",
    "            entity1_token = dependency_parser(sentence, entity1)\n",
    "            entity2_token = dependency_parser(sentence, entity2)\n",
    "        except Exception:\n",
    "            return 0\n",
    "        entity1_ancestors = list(entity1_token.ancestors)[::-1]\n",
    "        entity2_ancestors = list(entity2_token.ancestors)[::-1]\n",
    "        common_ancestor_index = self.get_common_ancestor(entity1_ancestors, entity2_ancestors) \n",
    "        shortest_path = entity1_ancestors[-1:common_ancestor_index:-1] + entity2_ancestors[common_ancestor_index:] \n",
    "        return len(shortest_path)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        shortest_path_matrix = [[self.calculate_shortest_path(row['sentence'], row['entity1'], row['entity2'])] for _, row in X.iterrows()]\n",
    "        shortest_path_matrix_scaled = StandardScaler().fit_transform(shortest_path_matrix)\n",
    "        return csr_matrix(shortest_path_matrix_scaled) # convert to sparse coded matrix\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7070x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7070 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_shortest_paths = ShortestPathFeature().fit_transform(X_train_preprocessed)\n",
    "X_shortest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class PathToRootFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, entity):\n",
    "        self.entity = entity\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        path_list = [self.path_to_root_word(row['sentence'], row[self.entity]) for _, row in X.iterrows()]\n",
    "        path_strings = [' '.join(path) for path in path_list]\n",
    "        path_matrix = self.vectorizer.fit(path_strings)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        path_list = [self.path_to_root_word(row['sentence'], row[self.entity]) for _, row in X.iterrows()]\n",
    "        path_strings = [' '.join(path) for path in path_list]\n",
    "        path_matrix = self.vectorizer.fit_transform(path_strings)\n",
    "        scaler = StandardScaler()\n",
    "        path_matrix_scaled = scaler.fit_transform(path_matrix)\n",
    "        return path_matrix_scaled\n",
    "    \n",
    "    def path_to_root_word(self, sentence, entity):\n",
    "        try:\n",
    "            entity_token = dependency_parser(sentence, entity)\n",
    "            return [ancestor.dep_ for ancestor in entity_token.ancestors[:-1]]\n",
    "        except Exception:\n",
    "            return []\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_paths \u001b[38;5;241m=\u001b[39m \u001b[43mPathToRootFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m X_paths\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36mPathToRootFeature.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m path_strings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m path_list]\n\u001b[0;32m     16\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m---> 17\u001b[0m path_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_strings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m     20\u001b[0m path_matrix_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(path_matrix)\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "X_paths = PathToRootFeature('entity1').fit_transform(X_train_preprocessed)\n",
    "X_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer class to generate semantic embeddings for phrases using pre-trained Word2Vec embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    column: The column containing the phrases to generate embeddings for.\n",
    "    word2vec: Pre-trained Word2Vec embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        self.word2vec = word2vec\n",
    "    \n",
    "    def get_phrase_context(self, phrase):\n",
    "        \"\"\"\n",
    "        Generate the semantic embedding context for a given phrase.\n",
    "        \n",
    "        Parameters:\n",
    "        phrase: The input phrase to generate the embedding for.\n",
    "        \n",
    "        Returns:\n",
    "        The semantic embedding context for the input phrase.\n",
    "        \"\"\"\n",
    "        phrase_tokens = tokenizer(phrase) # tokenize\n",
    "        context_vectors = []\n",
    "        \n",
    "        for token in phrase_tokens:\n",
    "            if token in self.word2vec:\n",
    "                token_vector = self.word2vec[token] # get embedding\n",
    "                context_vectors.append(token_vector)\n",
    "        \n",
    "        if len(context_vectors) == 0:\n",
    "            return np.zeros(300)  # Assuming word vectors are of size 300\n",
    "        \n",
    "        phrase_context = np.mean(context_vectors, axis=0) # create context by averaging the word vectors\n",
    "        return phrase_context\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform method to generate semantic embeddings for all phrases in the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        X: Input data containing the phrases.\n",
    "        \n",
    "        Returns:\n",
    "        Sparse matrix of semantic embeddings for the input phrases.\n",
    "        \"\"\"\n",
    "        return csr_matrix([self.get_phrase_context(phrase) for phrase in X[self.column]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7070x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1645530 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_between_entities_embedded = SemanticEmbeddings(column='tokens_between_entities').fit_transform(X_train_preprocessed)\n",
    "tokens_between_entities_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Combine extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_distances_feat = EntityDistance(absolute_values=True)\n",
    "\n",
    "sentences_tfidf_feat = TfidfFeatures(max_features=5000, ngram_range=(1,3)) # 5000 features\n",
    "\n",
    "# pos_feat = POSFeatures() #Removed\n",
    "\n",
    "shortest_path_feature = ShortestPathFeature()\n",
    "\n",
    "tokens_between_entities_feat = SemanticEmbeddings(column='tokens_between_entities') # 300 features\n",
    "\n",
    "entity1_neighbourhood_feat = SemanticEmbeddings(column='entity1_neighbourhood') # 300 features\n",
    "\n",
    "entity2_neighbourhood_feat = SemanticEmbeddings(column='entity2_neighbourhood') # 300 features\n",
    "\n",
    "\n",
    "combined_features = FeatureUnion([\n",
    "        (\"entity_distances_feat\", entity_distances_feat),\n",
    "        (\"sentences_tfidf_feat\", sentences_tfidf_feat),\n",
    "        (\"shortest_path_feature\", shortest_path_feature),\n",
    "        (\"tokens_between_entities_feat\", tokens_between_entities_feat),\n",
    "        (\"entity1_neighbourhood_feat\", entity1_neighbourhood_feat),\n",
    "        (\"entity2_neighbourhood_feat\", entity2_neighbourhood_feat)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized parts of speech feature did not provide good results, probably because the structure is not encapsulated properly by simple vectorization. Therefore, it was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7070x5902 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6063011 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features = combined_features.fit(X_train_preprocessed).transform(X_train_preprocessed)\n",
    "X_train_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform grid search over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Takes a while. Skip to Train section for training with tuned hyperparameters\n",
    "\n",
    "# Define base SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Create OneVsRestClassifier\n",
    "ovr_classifier = OneVsRestClassifier(svm_classifier, n_jobs=-1)\n",
    "\n",
    "# define a pipeline with feature extraction and the svm classifier\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"ovr_classifier\", ovr_classifier)])\n",
    "\n",
    "parameter_grid = dict(\n",
    "    features__sentences_tfidf_feat__max_features=[2000, 3000, 5000],\n",
    "    features__sentences_tfidf_feat__ngram_range=[(1,2), (1,3), (1,4)],\n",
    "    features__entity_distances_feat__absolute_values=[True,False],\n",
    "    ovr_classifier__estimator__C=[1.1, 1.2],\n",
    "    ovr_classifier__estimator__gamma=[\"scale\",\"auto\"],\n",
    "    ovr_classifier__estimator__probability=[True,False],\n",
    "    ovr_classifier__estimator__shrinking=[True,False],\n",
    "    ovr_classifier__estimator__break_ties=[True,False],\n",
    "    ovr_classifier__estimator__kernel=[\"rbf\", \"sigmoid\", \"linear\"],\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameter_grid, verbose=10)\n",
    "grid_search.fit(X_train_preprocessed[:100], Y_train[:100,:])\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameterization, train and validate model with the found hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;entity_distances_feat&#x27;,\n",
       "                                                 EntityDistance()),\n",
       "                                                (&#x27;sentences_tfidf_feat&#x27;,\n",
       "                                                 TfidfFeatures(max_features=3000)),\n",
       "                                                (&#x27;shortest_path_feature&#x27;,\n",
       "                                                 ShortestPathFeature()),\n",
       "                                                (&#x27;tokens_between_entities_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;tokens_between_entities&#x27;)),\n",
       "                                                (&#x27;entity1_neighbourhood_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;entity1_neighbourhood&#x27;)),\n",
       "                                                (&#x27;entity2_neighbourhood_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;entity2_neighbourhood&#x27;))])),\n",
       "                (&#x27;ovr_classifier&#x27;,\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.1, break_ties=True,\n",
       "                                                   kernel=&#x27;linear&#x27;,\n",
       "                                                   probability=True)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;entity_distances_feat&#x27;,\n",
       "                                                 EntityDistance()),\n",
       "                                                (&#x27;sentences_tfidf_feat&#x27;,\n",
       "                                                 TfidfFeatures(max_features=3000)),\n",
       "                                                (&#x27;shortest_path_feature&#x27;,\n",
       "                                                 ShortestPathFeature()),\n",
       "                                                (&#x27;tokens_between_entities_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;tokens_between_entities&#x27;)),\n",
       "                                                (&#x27;entity1_neighbourhood_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;entity1_neighbourhood&#x27;)),\n",
       "                                                (&#x27;entity2_neighbourhood_feat&#x27;,\n",
       "                                                 SemanticEmbeddings(column=&#x27;entity2_neighbourhood&#x27;))])),\n",
       "                (&#x27;ovr_classifier&#x27;,\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.1, break_ties=True,\n",
       "                                                   kernel=&#x27;linear&#x27;,\n",
       "                                                   probability=True)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;entity_distances_feat&#x27;, EntityDistance()),\n",
       "                               (&#x27;sentences_tfidf_feat&#x27;,\n",
       "                                TfidfFeatures(max_features=3000)),\n",
       "                               (&#x27;shortest_path_feature&#x27;, ShortestPathFeature()),\n",
       "                               (&#x27;tokens_between_entities_feat&#x27;,\n",
       "                                SemanticEmbeddings(column=&#x27;tokens_between_entities&#x27;)),\n",
       "                               (&#x27;entity1_neighbourhood_feat&#x27;,\n",
       "                                SemanticEmbeddings(column=&#x27;entity1_neighbourhood&#x27;)),\n",
       "                               (&#x27;entity2_neighbourhood_feat&#x27;,\n",
       "                                SemanticEmbeddings(column=&#x27;entity2_neighbourhood&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>entity_distances_feat</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EntityDistance</label><div class=\"sk-toggleable__content\"><pre>EntityDistance()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>sentences_tfidf_feat</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfFeatures</label><div class=\"sk-toggleable__content\"><pre>TfidfFeatures(max_features=3000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>shortest_path_feature</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ShortestPathFeature</label><div class=\"sk-toggleable__content\"><pre>ShortestPathFeature()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tokens_between_entities_feat</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SemanticEmbeddings</label><div class=\"sk-toggleable__content\"><pre>SemanticEmbeddings(column=&#x27;tokens_between_entities&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>entity1_neighbourhood_feat</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SemanticEmbeddings</label><div class=\"sk-toggleable__content\"><pre>SemanticEmbeddings(column=&#x27;entity1_neighbourhood&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>entity2_neighbourhood_feat</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SemanticEmbeddings</label><div class=\"sk-toggleable__content\"><pre>SemanticEmbeddings(column=&#x27;entity2_neighbourhood&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ovr_classifier: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=SVC(C=1.1, break_ties=True, kernel=&#x27;linear&#x27;,\n",
       "                                  probability=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1.1, break_ties=True, kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1.1, break_ties=True, kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('entity_distances_feat',\n",
       "                                                 EntityDistance()),\n",
       "                                                ('sentences_tfidf_feat',\n",
       "                                                 TfidfFeatures(max_features=3000)),\n",
       "                                                ('shortest_path_feature',\n",
       "                                                 ShortestPathFeature()),\n",
       "                                                ('tokens_between_entities_feat',\n",
       "                                                 SemanticEmbeddings(column='tokens_between_entities')),\n",
       "                                                ('entity1_neighbourhood_feat',\n",
       "                                                 SemanticEmbeddings(column='entity1_neighbourhood')),\n",
       "                                                ('entity2_neighbourhood_feat',\n",
       "                                                 SemanticEmbeddings(column='entity2_neighbourhood'))])),\n",
       "                ('ovr_classifier',\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.1, break_ties=True,\n",
       "                                                   kernel='linear',\n",
       "                                                   probability=True)))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and Combine features\n",
    "\n",
    "entity_distances_feat = EntityDistance(absolute_values=False)\n",
    "\n",
    "sentences_tfidf_feat = TfidfFeatures(max_features=3000, ngram_range=(1,3)) # 3000 features\n",
    "\n",
    "shortest_path_feature = ShortestPathFeature()\n",
    "\n",
    "tokens_between_entities_feat = SemanticEmbeddings(column='tokens_between_entities') # 300 features\n",
    "\n",
    "entity1_neighbourhood_feat = SemanticEmbeddings(column='entity1_neighbourhood') # 300 features\n",
    "\n",
    "entity2_neighbourhood_feat = SemanticEmbeddings(column='entity2_neighbourhood') # 300 features\n",
    "\n",
    "\n",
    "combined_features = FeatureUnion([\n",
    "        (\"entity_distances_feat\", entity_distances_feat),\n",
    "        (\"sentences_tfidf_feat\", sentences_tfidf_feat),\n",
    "        (\"shortest_path_feature\", shortest_path_feature),\n",
    "        (\"tokens_between_entities_feat\", tokens_between_entities_feat),\n",
    "        (\"entity1_neighbourhood_feat\", entity1_neighbourhood_feat),\n",
    "        (\"entity2_neighbourhood_feat\", entity2_neighbourhood_feat)\n",
    "    ])\n",
    "\n",
    "# Define base SVM classifier\n",
    "svm_classifier = SVC(C=1.1, break_ties=True, gamma='scale',kernel='linear',probability=True, shrinking=True)\n",
    "# Create OneVsRestClassifier\n",
    "ovr_classifier = OneVsRestClassifier(svm_classifier)\n",
    "# define a pipeline with feature extraction and the svm classifier\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"ovr_classifier\", ovr_classifier)])\n",
    "\n",
    "# Train the OneVsRestClassifier\n",
    "pipeline.fit(X_train_preprocessed, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8521923620933521\n",
      "F1 Score: 0.9181564064226744\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print training accuracy\n",
    "Y_train_pred = pipeline.predict(X_train_preprocessed)\n",
    "accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "f1_score_result = f1_score(Y_train, Y_train_pred, average='micro')\n",
    "print(\"F1 Score:\", f1_score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4772382397572079\n",
      "F1 Score: 0.6253934942287512\n",
      "\n",
      "Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                       brand       0.82      0.70      0.75        33\n",
      "           business_division       0.50      0.33      0.40         3\n",
      "                 chairperson       1.00      0.17      0.29         6\n",
      "     chief_executive_officer       0.67      0.38      0.48        32\n",
      "                     creator       0.67      0.53      0.59        19\n",
      "                    currency       1.00      1.00      1.00         9\n",
      "                   developer       0.44      0.57      0.50        21\n",
      "          director_/_manager       1.00      0.25      0.40         4\n",
      "              distributed_by       1.00      0.24      0.38        17\n",
      "         distribution_format       0.75      0.55      0.63        11\n",
      "                    employer       0.72      0.61      0.66        87\n",
      "                  founded_by       0.52      0.29      0.37        45\n",
      "       headquarters_location       0.69      0.75      0.71       114\n",
      "                    industry       0.77      0.72      0.74       213\n",
      "                  legal_form       0.75      0.72      0.74        29\n",
      "       location_of_formation       0.53      0.51      0.52        39\n",
      "                manufacturer       0.57      0.52      0.54        50\n",
      "                   member_of       0.70      0.50      0.58        14\n",
      "                    operator       0.56      0.47      0.51        19\n",
      "        original_broadcaster       0.75      0.69      0.72        13\n",
      "                    owned_by       0.56      0.33      0.41       160\n",
      "                    owner_of       0.59      0.45      0.51        94\n",
      "         parent_organization       0.60      0.34      0.43       115\n",
      "                    platform       1.00      0.50      0.67         4\n",
      "               position_held       0.88      0.86      0.87        50\n",
      "product_or_material_produced       0.70      0.74      0.72       219\n",
      "                   publisher       0.00      0.00      0.00         2\n",
      "              stock_exchange       0.80      0.88      0.84        51\n",
      "                  subsidiary       0.64      0.42      0.51        81\n",
      "\n",
      "                   micro avg       0.69      0.58      0.63      1554\n",
      "                   macro avg       0.70      0.52      0.57      1554\n",
      "                weighted avg       0.68      0.58      0.61      1554\n",
      "                 samples avg       0.57      0.60      0.57      1554\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# using dev files as  validation set\n",
    "val_sentences = read_file(\"./datasets/dev.sent\")\n",
    "val_relation_dicts = read_triplets(\"./datasets/dev.tup\")\n",
    "X_val, Y_val, _ = extract_X_Y(sentences=val_sentences, relation_dicts=val_relation_dicts)\n",
    "# preprocess X\n",
    "X_val_preprocessed = preprocessing(X_val)\n",
    "# Predictions\n",
    "Y_val_pred = pipeline.predict(X_val_preprocessed)\n",
    "accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "f1_score_result = f1_score(Y_val, Y_val_pred, average='micro')\n",
    "print(\"F1 Score:\", f1_score_result)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Y_val, Y_val_pred,target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and extract X & Y\n",
    "test_sentences = read_file(\"./datasets/test.sent\")\n",
    "test_relation_dicts = read_triplets(\"./datasets/test.tup\")\n",
    "X_test, Y_test, _ = extract_X_Y(sentences=test_sentences, relation_dicts=test_relation_dicts)\n",
    "# preprocess X\n",
    "X_test_preprocessed = preprocessing(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "(1277, 29)\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "Y_pred = pipeline.predict(X_test_preprocessed)\n",
    "# Y_pred = model.predict(X_test_features)\n",
    "print(Y_pred[0])\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4729835552075176\n",
      "F1 Score: 0.6284437825763217\n",
      "\n",
      "Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                       brand       0.91      0.53      0.67        19\n",
      "           business_division       1.00      0.80      0.89        10\n",
      "                 chairperson       1.00      0.89      0.94        19\n",
      "     chief_executive_officer       0.67      0.64      0.65        28\n",
      "                     creator       1.00      0.67      0.80        21\n",
      "                    currency       0.78      0.88      0.82         8\n",
      "                   developer       0.72      0.58      0.64        48\n",
      "          director_/_manager       1.00      0.30      0.46        10\n",
      "              distributed_by       0.62      0.36      0.45        14\n",
      "         distribution_format       0.89      0.62      0.73        13\n",
      "                    employer       0.74      0.78      0.76        80\n",
      "                  founded_by       0.73      0.54      0.62        41\n",
      "       headquarters_location       0.71      0.71      0.71        99\n",
      "                    industry       0.71      0.74      0.72       170\n",
      "                  legal_form       0.71      0.25      0.37        20\n",
      "       location_of_formation       0.53      0.40      0.46        40\n",
      "                manufacturer       0.61      0.34      0.44        50\n",
      "                   member_of       0.83      0.71      0.77         7\n",
      "                    operator       0.70      0.70      0.70        23\n",
      "        original_broadcaster       0.17      0.10      0.12        10\n",
      "                    owned_by       0.61      0.36      0.45       167\n",
      "                    owner_of       0.79      0.47      0.59       118\n",
      "         parent_organization       0.56      0.40      0.46       123\n",
      "                    platform       0.67      0.67      0.67         6\n",
      "               position_held       0.79      0.70      0.75        27\n",
      "product_or_material_produced       0.74      0.73      0.74       187\n",
      "                   publisher       0.00      0.00      0.00         6\n",
      "              stock_exchange       0.81      0.84      0.82        25\n",
      "                  subsidiary       0.67      0.40      0.50        99\n",
      "\n",
      "                   micro avg       0.70      0.57      0.63      1488\n",
      "                   macro avg       0.71      0.56      0.61      1488\n",
      "                weighted avg       0.70      0.57      0.62      1488\n",
      "                 samples avg       0.56      0.58      0.56      1488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Clint\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "f1_score_result = f1_score(Y_test, Y_pred, average='micro')\n",
    "print(\"F1 Score:\", f1_score_result)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred,target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(sentence, entity1, entity2):\n",
    "  X = np.array([[sentence,entity1,entity2]])\n",
    "  X_preprocessed = preprocessing(X)\n",
    "  Y_pred = pipeline.predict(X_preprocessed)\n",
    "  return mlb.inverse_transform(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subsidiary',)]\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"Wednesday, July 8, 2015 10:30AM IST (5:00AM GMT) Rimini Street Comment on Oracle Litigation Las Vegas, United States Rimini Street, Inc., the leading independent provider of enterprise software support for SAP AGs (NYSE:SAP) Business Suite and BusinessObjects software and Oracle Corporations (NYSE:ORCL) Siebel , PeopleSoft , JD Edwards , E-Business Suite , Oracle Database , Hyperion and Oracle Retail software, today issued a statement on the Oracle litigation.\"\n",
    "entity1, entity2 = \"PeopleSoft\",  \"JD Edwards\"\n",
    "print(predict_with_model(sample_sentence,entity1, entity2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
